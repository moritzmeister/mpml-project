{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from DataUtils import DataUtil\n",
    "import time\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataUtil = DataUtil(sc, 'data/spam.data.txt', 'data/mean_std.txt', True)\n",
    "rdd = dataUtil.read(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.zipWithIndex().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(data):\n",
    "    def f(y, pred):\n",
    "        if y == pred:\n",
    "            return 1,0,0,0 if y == 1 else 0,1,0,0\n",
    "        elif pred == 1:\n",
    "            return 0,0,1,0\n",
    "        else:\n",
    "            return 0,0,0,1\n",
    "    tp, tn, fp, fn = data.map(lambda x: f(x[2], x[4])).reduce(lambda a, b: tuple(map(sum, zip(a, b))))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "class ParallelLogReg():\n",
    "    \n",
    "    def __init__(self, sc, dataUtils, iterations, learning_rate, lambda_reg, fit_intercept, threshold=0.5):\n",
    "        self.dataUtils = dataUtils\n",
    "        # do we need to broadcast these?\n",
    "        self.iterations = iterations\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.data = self.dataUtils.read(sc)\n",
    "        self.numberObservations = self.data.count()\n",
    "        self.numberFeatures = 56 #len(self.data.first()[0])\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def __add_intercept(self):\n",
    "        self.data = self.data.map(lambda x: (1, x[0], x[1]))\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __predict_y(self, w, x):\n",
    "        return self.__sigmoid(w[1].dot(x[1]) + w[0] * x[0])\n",
    "    \n",
    "    def calculateLoss(self):\n",
    "        return 1\n",
    "    \n",
    "    def train(self):\n",
    "        import sys\n",
    "        eps = sys.float_info.epsilon\n",
    "        self.__add_intercept()\n",
    "        \n",
    "        lambda_reg = self.lambda_reg\n",
    "        m = self.numberObservations\n",
    "        lr = self.lr\n",
    "        threshold = self.threshold\n",
    "        \n",
    "        # initialize the weights\n",
    "        # w[0]: bias weight\n",
    "        # w[1]: rest of the weights\n",
    "        w = (0, np.zeros(self.numberFeatures))\n",
    "        \n",
    "        # initialize prediction to rdd\n",
    "        # x[0]: bias/intercept\n",
    "        # x[1]: rest features\n",
    "        # x[2]: true y\n",
    "        # adding x[3]: predicted y\n",
    "        data = self.data.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(np.dot(x[1], w[1]) + x[0] * w[0]))))).cache()\n",
    "                \n",
    "        for i in range(self.iterations):\n",
    "            start = time.time()\n",
    "            # compute derivatives\n",
    "            temp = data.map(lambda x: ((x[3] - x[2]) * x[0], (x[3] - x[2]) * x[1])) \\\n",
    "                         .reduce(lambda a,b: (a[0] + b[0], a[1] + b[1]))            \n",
    "            dw = (temp[0]/m, (temp[1]/m) + (lambda_reg/m) * w[1])\n",
    "            derivatives_end_time = time.time()\n",
    "\n",
    "            # update weights\n",
    "            w = (w[0] - lr * dw[0], w[1] - lr * dw[1])\n",
    "            update_weights_end_time = time.time()\n",
    "\n",
    "            # update prediction\n",
    "            data = data.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(np.dot(x[1], w[1]) + x[0] * w[0]))))).cache()\n",
    "            update_prediction_end_time = time.time()\n",
    "\n",
    "            if (i%10 == 0):\n",
    "                loss = data.map(lambda x: x[2] * np.log(x[3] + eps) + (1 - x[2]) * np.log(1 - x[3] + eps)) \\\n",
    "                     .reduce(lambda a,b: a + b)\n",
    "                loss_end_time = time.time()\n",
    "\n",
    "                loss = -(1/self.numberObservations) * loss + (self.lambda_reg/(2*self.numberObservations)) * np.sum(w[1]**2)\n",
    "                end = time.time()\n",
    "                print('Loss: ' + str(loss))\n",
    "                print('Total time: ' + str(end - start))\n",
    "                print('Compute derivatives time: ' + str(derivatives_end_time - start))\n",
    "                print('Update weights time: ' + str(update_weights_end_time - derivatives_end_time))\n",
    "                print('Update prediciton time: ' + str(update_prediction_end_time - update_weights_end_time))\n",
    "                print('Compute loss time: ' + str(loss_end_time - update_prediction_end_time))\n",
    "                \n",
    "                data = data.map(lambda x: (x[0], x[1], x[2], x[3], 1 if x[3] >= threshold else 0)).cache()\n",
    "                precision, recall, f1, accuracy = stats(data)\n",
    "                print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(precision, recall, f1, accuracy))\n",
    "                print('\\n')\n",
    "            \n",
    "            \n",
    "                \n",
    "        data = data.map(lambda x: (x[0], x[1], x[2], x[3], 1 if x[3] >= 0.5 else 0))\n",
    "        \n",
    "        self.acc = data.map(lambda x: 1 if x[2] == x[4] else 0) \\\n",
    "                            .reduce(lambda a,b: a+b) / self.numberObservations\n",
    "            \n",
    "        return w\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = ParallelLogReg(sc, dataUtil, 100, 0.1, 0.1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6503220242271294\n",
      "Total time: 0.44766926765441895\n",
      "Compute derivatives time: 0.26760101318359375\n",
      "Update weights time: 1.3828277587890625e-05\n",
      "Update prediciton time: 0.010751724243164062\n",
      "Compute loss time: 0.16924691200256348\n",
      "Precision: 0.9370725034199726, Recall: 0.9502890173410404, F1: 0.9436344851337389, Accuracy: 0.8932840686807216\n",
      "\n",
      "\n",
      "Loss: 0.5221875677942007\n",
      "Total time: 0.31861329078674316\n",
      "Compute derivatives time: 0.13772130012512207\n",
      "Update weights time: 1.2874603271484375e-05\n",
      "Update prediciton time: 0.014154911041259766\n",
      "Compute loss time: 0.1666562557220459\n",
      "Precision: 0.9438202247191011, Recall: 0.9449035812672176, F1: 0.9443615922909258, Accuracy: 0.8945881330145621\n",
      "\n",
      "\n",
      "Loss: 0.4581797205838391\n",
      "Total time: 0.3738396167755127\n",
      "Compute derivatives time: 0.20208072662353516\n",
      "Update weights time: 1.2874603271484375e-05\n",
      "Update prediciton time: 0.016008615493774414\n",
      "Compute loss time: 0.15566635131835938\n",
      "Precision: 0.9489772466099747, Recall: 0.9429093400319708, F1: 0.9459335624284078, Accuracy: 0.8974136057378831\n",
      "\n",
      "\n",
      "Loss: 0.4194697537285747\n",
      "Total time: 0.4108617305755615\n",
      "Compute derivatives time: 0.16255855560302734\n",
      "Update weights time: 1.4543533325195312e-05\n",
      "Update prediciton time: 0.024796247482299805\n",
      "Compute loss time: 0.22342395782470703\n",
      "Precision: 0.9504950495049505, Recall: 0.9411764705882353, F1: 0.9458128078817734, Accuracy: 0.897196261682243\n",
      "\n",
      "\n",
      "Loss: 0.3933220047850806\n",
      "Total time: 0.3631110191345215\n",
      "Compute derivatives time: 0.15885257720947266\n",
      "Update weights time: 1.3828277587890625e-05\n",
      "Update prediciton time: 0.015427589416503906\n",
      "Compute loss time: 0.18875503540039062\n",
      "Precision: 0.9531718569780854, Recall: 0.9395179627103228, F1: 0.9462956601397, Accuracy: 0.8980656379048033\n",
      "\n",
      "\n",
      "Loss: 0.37435348725683426\n",
      "Total time: 0.36584997177124023\n",
      "Compute derivatives time: 0.1945323944091797\n",
      "Update weights time: 1.2874603271484375e-05\n",
      "Update prediciton time: 0.015287399291992188\n",
      "Compute loss time: 0.15594696998596191\n",
      "Precision: 0.9556888991460881, Recall: 0.9392152415513721, F1: 0.9473804621368108, Accuracy: 0.900021734405564\n",
      "\n",
      "\n",
      "Loss: 0.35989145975858117\n",
      "Total time: 0.3815615177154541\n",
      "Compute derivatives time: 0.19615745544433594\n",
      "Update weights time: 1.1682510375976562e-05\n",
      "Update prediciton time: 0.010522127151489258\n",
      "Compute loss time: 0.17480087280273438\n",
      "Precision: 0.9566020313942751, Recall: 0.9390437344210287, F1: 0.947741566609491, Accuracy: 0.9006737665724842\n",
      "\n",
      "\n",
      "Loss: 0.34845345961766216\n",
      "Total time: 0.6568639278411865\n",
      "Compute derivatives time: 0.3081991672515869\n",
      "Update weights time: 1.0967254638671875e-05\n",
      "Update prediciton time: 0.0277249813079834\n",
      "Compute loss time: 0.31863975524902344\n",
      "Precision: 0.9579385255373237, Recall: 0.9379950214980765, F1: 0.9478618797164418, Accuracy: 0.9008911106281243\n",
      "\n",
      "\n",
      "Loss: 0.3391484146299768\n",
      "Total time: 0.378558874130249\n",
      "Compute derivatives time: 0.18519926071166992\n",
      "Update weights time: 1.3589859008789062e-05\n",
      "Update prediciton time: 0.01377248764038086\n",
      "Compute loss time: 0.17950105667114258\n",
      "Precision: 0.9593251675525768, Recall: 0.9380790960451978, F1: 0.948583180987203, Accuracy: 0.9021951749619648\n",
      "\n",
      "\n",
      "Loss: 0.331407130859889\n",
      "Total time: 0.41426515579223633\n",
      "Compute derivatives time: 0.20853757858276367\n",
      "Update weights time: 1.33514404296875e-05\n",
      "Update prediciton time: 0.01679396629333496\n",
      "Compute loss time: 0.18884754180908203\n",
      "Precision: 0.960425827354779, Recall: 0.9367945823927766, F1: 0.9484630327962519, Accuracy: 0.9019778309063247\n",
      "\n",
      "\n",
      "Loss: 0.32484830123911546\n",
      "Total time: 0.4391744136810303\n",
      "Compute derivatives time: 0.20321083068847656\n",
      "Update weights time: 1.33514404296875e-05\n",
      "Update prediciton time: 0.014215469360351562\n",
      "Compute loss time: 0.221665620803833\n",
      "Precision: 0.961574074074074, Recall: 0.936640360766629, F1: 0.9489434608794973, Accuracy: 0.902847207128885\n",
      "\n",
      "\n",
      "Loss: 0.31920653228548723\n",
      "Total time: 0.33930063247680664\n",
      "Compute derivatives time: 0.14273381233215332\n",
      "Update weights time: 1.3113021850585938e-05\n",
      "Update prediciton time: 0.014612436294555664\n",
      "Compute loss time: 0.18187355995178223\n",
      "Precision: 0.9618143948160148, Recall: 0.9368800721370604, F1: 0.9491835103345894, Accuracy: 0.9032818952401652\n",
      "\n",
      "\n",
      "Loss: 0.31429121966921003\n",
      "Total time: 0.364851713180542\n",
      "Compute derivatives time: 0.15630674362182617\n",
      "Update weights time: 1.3113021850585938e-05\n",
      "Update prediciton time: 0.019122838973999023\n",
      "Compute loss time: 0.18933439254760742\n",
      "Precision: 0.9622859787135586, Recall: 0.9371338440739072, F1: 0.949543378995434, Accuracy: 0.9039339274070854\n",
      "\n",
      "\n",
      "Loss: 0.30996183371216335\n",
      "Total time: 0.3908393383026123\n",
      "Compute derivatives time: 0.2040419578552246\n",
      "Update weights time: 1.3589859008789062e-05\n",
      "Update prediciton time: 0.012567520141601562\n",
      "Compute loss time: 0.17415308952331543\n",
      "Precision: 0.9627314814814815, Recall: 0.9367117117117117, F1: 0.9495433789954338, Accuracy: 0.9039339274070854\n",
      "\n",
      "\n",
      "Loss: 0.30611242960061724\n",
      "Total time: 0.38094305992126465\n",
      "Compute derivatives time: 0.1856677532196045\n",
      "Update weights time: 1.4543533325195312e-05\n",
      "Update prediciton time: 0.014414072036743164\n",
      "Compute loss time: 0.18079042434692383\n",
      "Precision: 0.9636658180976626, Recall: 0.936993699369937, F1: 0.9501426126640046, Accuracy: 0.9050206476852858\n",
      "\n",
      "\n",
      "Loss: 0.30266158855580405\n",
      "Total time: 0.42665886878967285\n",
      "Compute derivatives time: 0.25045013427734375\n",
      "Update weights time: 1.4066696166992188e-05\n",
      "Update prediciton time: 0.014354467391967773\n",
      "Compute loss time: 0.1617729663848877\n",
      "Precision: 0.9641369736233225, Recall: 0.937246963562753, F1: 0.9505018248175183, Accuracy: 0.905672679852206\n",
      "\n",
      "\n",
      "Loss: 0.2995456845167386\n",
      "Total time: 0.33402037620544434\n",
      "Compute derivatives time: 0.1795201301574707\n",
      "Update weights time: 1.621246337890625e-05\n",
      "Update prediciton time: 0.01594400405883789\n",
      "Compute loss time: 0.13848066329956055\n",
      "Precision: 0.9643601018282805, Recall: 0.937036204182595, F1: 0.9505018248175183, Accuracy: 0.905672679852206\n",
      "\n",
      "\n",
      "Loss: 0.2967142570874724\n",
      "Total time: 0.3608555793762207\n",
      "Compute derivatives time: 0.16460490226745605\n",
      "Update weights time: 1.3589859008789062e-05\n",
      "Update prediciton time: 0.017015457153320312\n",
      "Compute loss time: 0.17916107177734375\n",
      "Precision: 0.9643518518518519, Recall: 0.9368113334832472, F1: 0.9503821147484887, Accuracy: 0.905455335796566\n",
      "\n",
      "\n",
      "Loss: 0.29412675827114254\n",
      "Total time: 0.38028383255004883\n",
      "Compute derivatives time: 0.22728300094604492\n",
      "Update weights time: 1.33514404296875e-05\n",
      "Update prediciton time: 0.011679887771606445\n",
      "Compute loss time: 0.14124274253845215\n",
      "Precision: 0.9648148148148148, Recall: 0.9368397392672511, F1: 0.950621507583533, Accuracy: 0.9058900239078461\n",
      "\n",
      "\n",
      "Loss: 0.29175021861435124\n",
      "Total time: 0.33843564987182617\n",
      "Compute derivatives time: 0.17904996871948242\n",
      "Update weights time: 1.3589859008789062e-05\n",
      "Update prediciton time: 0.013142108917236328\n",
      "Compute loss time: 0.1461644172668457\n",
      "Precision: 0.9652777777777778, Recall: 0.9368681195237025, F1: 0.9508607912438719, Accuracy: 0.9063247120191262\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.4283915364779883,\n",
       " array([-0.00622156, -0.06707946,  0.12603675,  0.10679152,  0.28608055,\n",
       "         0.20375194,  0.49958148,  0.24671533,  0.22264238,  0.09935942,\n",
       "         0.12813877, -0.10040145,  0.05902925,  0.04332017,  0.18387406,\n",
       "         0.36372831,  0.28676633,  0.20048329,  0.15539207,  0.22677141,\n",
       "         0.31132481,  0.19344826,  0.4381361 ,  0.26738814, -0.29162514,\n",
       "        -0.22380624, -0.25198122, -0.05837027, -0.11482588, -0.13995595,\n",
       "        -0.07090135, -0.03828431, -0.15286331, -0.03645856, -0.11456854,\n",
       "        -0.02073074, -0.1365954 , -0.05547121, -0.12921777, -0.00276266,\n",
       "        -0.11337995, -0.20190582, -0.11778833, -0.13805032, -0.23019683,\n",
       "        -0.22506779, -0.08137009, -0.11471476, -0.1139385 , -0.04269744,\n",
       "        -0.05166024,  0.33324663,  0.4674863 ,  0.11167972,  0.12264795,\n",
       "         0.28245354]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
