{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Implementation and Evaluation of Logistic Regression\n",
    "### Massively Parallel Machine Learning (2018-19)\n",
    "---\n",
    "#### Students: \n",
    "Marcin Paszkiewicz  \n",
    "Moritz Meister\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import libraries and initialize the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define helper class to read data into an RDD and simultaneously standardize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtil:\n",
    "    def __init__(self, sc, inputPath='data/spam.data.txt', statsInputPath='data/mean_std.txt', standardize=True):\n",
    "        self.inputPath = inputPath\n",
    "        self.standardize = standardize\n",
    "        self.mean_std = pd.read_csv(statsInputPath, delimiter=',', header=None)\n",
    "        self.mean = sc.broadcast(self.mean_std.iloc[:,0].values.astype(float))\n",
    "        self.std = sc.broadcast(self.mean_std.iloc[:,1].values.astype(float))\n",
    "        \n",
    "        \n",
    "    def read(self, sc):\n",
    "        if self.standardize:\n",
    "            return sc.textFile(self.inputPath).map(lambda x: np.asarray(x.split(' ')).astype(float)) \\\n",
    "                    .map(lambda x: (x[:56], x[57])) \\\n",
    "                    .map(lambda x: ((x[0] - self.mean.value[:56])/self.std.value[:56], x[1]))\n",
    "        else:\n",
    "            return sc.textFile(self.inputPath).map(lambda x: np.asarray(x.split(' ')).astype(float)) \\\n",
    "                    .map(lambda x: (x[:56], x[57]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Initialize the DataUtil helper and split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset: 4601, 56\n",
      "Train dataset: 3675, 56\n",
      "Test dataset: 926, 56\n"
     ]
    }
   ],
   "source": [
    "spam_dataset = DataUtil(sc, 'data/spam.data.txt', 'data/mean_std.txt', True).read(sc)\n",
    "train_set, test_set = spam_dataset.randomSplit(weights=[0.8, 0.2], seed=1)\n",
    "\n",
    "print(\"Whole dataset: {}, {}\".format(spam_dataset.count(), len(spam_dataset.first()[0])))\n",
    "print(\"Train dataset: {}, {}\".format(train_set.count(), len(train_set.first()[0])))\n",
    "print(\"Test dataset: {}, {}\".format(test_set.count(), len(test_set.first()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Functions that can't be packaged in a Python class since they need to be serializable in order to be passed to the Spark workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_function(y, h):\n",
    "    eps = sys.float_info.epsilon\n",
    "    return y * np.log(h + eps) + (1 - y) * np.log(1 - h + eps)\n",
    "\n",
    "def stats(data):\n",
    "    def f(y, pred):\n",
    "        if y == pred:\n",
    "            return 1,0,0,0 if y == 1 else 0,1,0,0\n",
    "        elif pred == 1:\n",
    "            return 0,0,1,0\n",
    "        else:\n",
    "            return 0,0,0,1\n",
    "    tp, tn, fp, fn = data.map(lambda x: f(x[2], x[4])).reduce(lambda a, b: tuple(map(sum, zip(a, b))))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Definition of our Class containing all the functionality for the parallelized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelLogReg():\n",
    "    \n",
    "    def __init__(self, data, iterations, learning_rate, lambda_reg):\n",
    "        self.data = data\n",
    "        self.numberObservations = self.data.count()\n",
    "        self.numberFeatures = len(self.data.first()[0])\n",
    "        self.iterations = iterations\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "    def __add_intercept(self, rdd):\n",
    "        return rdd.map(lambda x: (1, x[0], x[1]))\n",
    "        \n",
    "    def train(self, train_rdd=None, SGD=False, SGD_pct=0.5, threshold=0.5):\n",
    "        eps = sys.float_info.epsilon\n",
    "        \n",
    "        # initialize the weights\n",
    "        # w[0]: bias weight\n",
    "        # w[1]: rest of the weights\n",
    "        w = (0, np.zeros(self.numberFeatures))\n",
    "        \n",
    "        if train_rdd == None:\n",
    "            data = self.__add_intercept(self.data)\n",
    "        else:\n",
    "            data = self.__add_intercept(train_rdd)\n",
    "\n",
    "        # initialize prediction to rdd\n",
    "        # x[0]: bias/intercept\n",
    "        # x[1]: rest features\n",
    "        # x[2]: true y\n",
    "        # adding x[3]: predicted y\n",
    "        data = data.map(lambda x: (x[0], x[1], x[2], sigmoid(w[1].dot(x[1]) + w[0] * x[0])))\n",
    "        numObs = data.count()\n",
    "        \n",
    "        train = data.cache()\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            start = time.time()\n",
    "            \n",
    "            # sample for SGD\n",
    "            if SGD:\n",
    "                train = data.sample(False, SGD_pct).repartition(sc.defaultParallelism).cache()\n",
    "                if i == 0:\n",
    "                    numObs = train.count()\n",
    "            \n",
    "            # compute derivatives\n",
    "            temp = train.map(lambda x: ((x[3] - x[2]) * x[0], (x[3] - x[2]) * x[1])) \\\n",
    "                       .reduce(lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "            dw = (temp[0]/numObs, (temp[1]/numObs) + self.lambda_reg * w[1])\n",
    "            \n",
    "            # update weights\n",
    "            w = (w[0] - self.lr * dw[0], w[1] - self.lr * dw[1])\n",
    "            \n",
    "            # update prediction\n",
    "            train = train.map(lambda x: (x[0], x[1], x[2], sigmoid(w[1].dot(x[1]) + w[0] * x[0]))).cache()\n",
    "            \n",
    "            if (i % 5 == 0):\n",
    "                end = time.time()\n",
    "                \n",
    "                model_stats = self.validate(train, w, add_intercept=False)\n",
    "\n",
    "                print(\"Iteration: \" + str(i) + \", Total Time: \" + str(end - start))\n",
    "                print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(model_stats['precision'], model_stats['recall'], model_stats['f1'], model_stats['accuracy']))\n",
    "                print('Current loss: {}\\n'.format(model_stats['loss']))\n",
    "                \n",
    "        model_stats = self.validate(data, w, add_intercept=False)\n",
    "        \n",
    "        print('Training stats:')\n",
    "        print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(model_stats['precision'], model_stats['recall'], model_stats['f1'], model_stats['accuracy']))\n",
    "        print('Final training loss: {}\\n'.format(model_stats['loss']))\n",
    "            \n",
    "        return {**model_stats, 'w': w}\n",
    "    \n",
    "    def validate(self, val_rdd, w, threshold=0.5, add_intercept=True):\n",
    "        if add_intercept:\n",
    "            val_rdd = self.__add_intercept(val_rdd)\n",
    "        \n",
    "        numObs = val_rdd.count()\n",
    "        \n",
    "        # update prediction\n",
    "        val_rdd = val_rdd.map(lambda x: (x[0], x[1], x[2], sigmoid(w[1].dot(x[1]) + w[0] * x[0])))\n",
    "        \n",
    "        loss = val_rdd.map(lambda x: cost_function(x[2], x[3])) \\\n",
    "                    .reduce(lambda a,b: a + b)\n",
    "        loss = -(1/numObs) * loss + (self.lambda_reg/2) * np.sum(w[1]**2)\n",
    "        \n",
    "        # calculate stats\n",
    "        val_rdd = val_rdd.map(lambda x: (x[0], x[1], x[2], x[3], 1 if x[3] >= threshold else 0)).cache()\n",
    "        precision, recall, f1, accuracy = stats(val_rdd)\n",
    "        return {'loss': loss, 'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}\n",
    "    \n",
    "    def cross_validate(self, k, SGD=False, SGD_pct = 0.5):\n",
    "        \n",
    "        if k == 1:\n",
    "            print(\"Please choose a k > 1. Or use regular train function\")\n",
    "            return\n",
    "        \n",
    "        # even if it is ordered, this procedure should produce folds with similar distributions\n",
    "        dataWithIndex = self.data.zipWithIndex();\n",
    "        \n",
    "        model_stats = []\n",
    "        models = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            print(\"Fold: \" + str(i))\n",
    "            \n",
    "            fold = dataWithIndex.filter(lambda x: x[1] % k != i).repartition(sc.defaultParallelism) \\\n",
    "                                .map(lambda x: x[0])\n",
    "            \n",
    "            model = self.train(fold, SGD, SGD_pct)\n",
    "            models.append(model)\n",
    "            \n",
    "            val = dataWithIndex.filter(lambda x: x[1] % k == i).repartition(sc.defaultParallelism) \\\n",
    "                                .map(lambda x: x[0])\n",
    "            \n",
    "            val_stats = self.validate(val, model['w'])\n",
    "            model_stats.append(val_stats)\n",
    "            print('Validation stats:')\n",
    "            print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(val_stats['precision'], val_stats['recall'], val_stats['f1'], val_stats['accuracy']))\n",
    "            print('Loss: {}\\n'.format(val_stats['loss']))\n",
    "\n",
    "        avg_model_stats = {}\n",
    "        for key in model_stats[0].keys():\n",
    "            avg_model_stats['avg_' + key] = sum(stat[key] for stat in model_stats) / len(model_stats)\n",
    "\n",
    "        print('Averaged validation stats:')\n",
    "        print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(avg_model_stats['avg_precision'], avg_model_stats['avg_recall'], avg_model_stats['avg_f1'], avg_model_stats['avg_accuracy']))\n",
    "        print('Loss: {}'.format(avg_model_stats['avg_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Initialize a new Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = ParallelLogReg(data=train_set, iterations=200, learning_rate=0.1, lambda_reg=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Perform Cross-Validation with the above specified parameters and 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "logReg.cross_validate(10)\n",
    "print('Total time:')\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Train the model with 80% train set and the previously found best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Total Time: 0.9133286476135254\n",
      "Precision: 0.9399373754625676, Recall: 0.9532332563510393, F1: 0.9465386269170131, Accuracy: 0.8985034013605442\n",
      "Current loss: 0.6492381078517572\n",
      "\n",
      "Iteration: 5, Total Time: 0.22257184982299805\n",
      "Precision: 0.9488425264361247, Recall: 0.9496567505720824, F1: 0.9492494639027877, Accuracy: 0.9034013605442177\n",
      "Current loss: 0.5214280133698086\n",
      "\n",
      "Iteration: 10, Total Time: 0.23688960075378418\n",
      "Precision: 0.9535417264123889, Recall: 0.946484486194136, F1: 0.95, Accuracy: 0.9047619047619048\n",
      "Current loss: 0.4610221795060205\n",
      "\n",
      "Iteration: 15, Total Time: 0.22551560401916504\n",
      "Precision: 0.9548331415420023, Recall: 0.9434337691870381, F1: 0.9490992279096367, Accuracy: 0.9031292517006803\n",
      "Current loss: 0.4264075428524229\n",
      "\n",
      "Iteration: 20, Total Time: 0.1652381420135498\n",
      "Precision: 0.9574100719424461, Recall: 0.9432945846328324, F1: 0.9502999143101972, Accuracy: 0.9053061224489796\n",
      "Current loss: 0.40425820021823244\n",
      "\n",
      "Iteration: 25, Total Time: 0.22816133499145508\n",
      "Precision: 0.959136690647482, Recall: 0.9433908859326352, F1: 0.9511986301369864, Accuracy: 0.9069387755102041\n",
      "Current loss: 0.38904855769332913\n",
      "\n",
      "Iteration: 30, Total Time: 0.3157055377960205\n",
      "Precision: 0.9611063094209161, Recall: 0.9423728813559322, F1: 0.9516474112109541, Accuracy: 0.9077551020408163\n",
      "Current loss: 0.3780809246201987\n",
      "\n",
      "Iteration: 35, Total Time: 0.2436671257019043\n",
      "Precision: 0.9622478386167147, Recall: 0.9421557562076749, F1: 0.9520958083832335, Accuracy: 0.9085714285714286\n",
      "Current loss: 0.36988338153372124\n",
      "\n",
      "Iteration: 40, Total Time: 0.20206546783447266\n",
      "Precision: 0.9639041293676004, Recall: 0.9402816901408451, F1: 0.9519463852844717, Accuracy: 0.9082993197278911\n",
      "Current loss: 0.3635857955607794\n",
      "\n",
      "Iteration: 45, Total Time: 0.22204160690307617\n",
      "Precision: 0.9650086755349914, Recall: 0.9389420371412492, F1: 0.9517969195664575, Accuracy: 0.9080272108843538\n",
      "Current loss: 0.35864174248299563\n",
      "\n",
      "Iteration: 50, Total Time: 0.1975994110107422\n",
      "Precision: 0.9661751951431049, Recall: 0.93929173693086, F1: 0.9525438221462164, Accuracy: 0.9093877551020408\n",
      "Current loss: 0.35469150662674226\n",
      "\n",
      "Iteration: 55, Total Time: 0.25625157356262207\n",
      "Precision: 0.9670424978317432, Recall: 0.9393428812131424, F1: 0.9529914529914529, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.35148914744995036\n",
      "\n",
      "Iteration: 60, Total Time: 0.22580933570861816\n",
      "Precision: 0.967032967032967, Recall: 0.9390620612187588, F1: 0.9528422852258156, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.34886120494678713\n",
      "\n",
      "Iteration: 65, Total Time: 0.25232410430908203\n",
      "Precision: 0.9670138888888888, Recall: 0.9385004212299916, F1: 0.9525438221462162, Accuracy: 0.9093877551020408\n",
      "Current loss: 0.34668211649782804\n",
      "\n",
      "Iteration: 70, Total Time: 0.17615532875061035\n",
      "Precision: 0.9678726483357453, Recall: 0.9382716049382716, F1: 0.9528422852258156, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.3448589609158629\n",
      "\n",
      "Iteration: 75, Total Time: 0.16960477828979492\n",
      "Precision: 0.9687409551374819, Recall: 0.9383235211662462, F1: 0.9532896610652236, Accuracy: 0.9107482993197279\n",
      "Current loss: 0.3433216533541992\n",
      "\n",
      "Iteration: 80, Total Time: 0.17946505546569824\n",
      "Precision: 0.9690483077813132, Recall: 0.9389013452914798, F1: 0.9537366548042705, Accuracy: 0.9115646258503401\n",
      "Current loss: 0.3420164521959462\n",
      "\n",
      "Iteration: 85, Total Time: 0.15703654289245605\n",
      "Precision: 0.9690393518518519, Recall: 0.9386210762331838, F1: 0.9535876993166286, Accuracy: 0.9112925170068027\n",
      "Current loss: 0.3409015465683552\n",
      "\n",
      "Iteration: 90, Total Time: 0.19800996780395508\n",
      "Precision: 0.9696180555555556, Recall: 0.938655462184874, F1: 0.9538855678906918, Accuracy: 0.9118367346938776\n",
      "Current loss: 0.3399439892047657\n",
      "\n",
      "Iteration: 95, Total Time: 0.19276738166809082\n",
      "Precision: 0.9698899826288361, Recall: 0.938112573508821, F1: 0.9537366548042704, Accuracy: 0.9115646258503401\n",
      "Current loss: 0.33911752143004115\n",
      "\n",
      "Iteration: 100, Total Time: 0.1831655502319336\n",
      "Precision: 0.9701189440092834, Recall: 0.9361702127659575, F1: 0.9528422852258157, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.3384010030213328\n",
      "\n",
      "Iteration: 105, Total Time: 0.17169690132141113\n",
      "Precision: 0.9701189440092834, Recall: 0.9361702127659575, F1: 0.9528422852258157, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.33777726036845374\n",
      "\n",
      "Iteration: 110, Total Time: 0.1660778522491455\n",
      "Precision: 0.9704090513489991, Recall: 0.9361880772460117, F1: 0.9529914529914529, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.3372322290815616\n",
      "\n",
      "Iteration: 115, Total Time: 0.18285489082336426\n",
      "Precision: 0.9709639953542393, Recall: 0.9353846153846154, F1: 0.9528422852258157, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.33675430721791744\n",
      "\n",
      "Iteration: 120, Total Time: 0.18663763999938965\n",
      "Precision: 0.9712543554006968, Recall: 0.9354026845637584, F1: 0.9529914529914529, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.3363338613899264\n",
      "\n",
      "Iteration: 125, Total Time: 0.2147064208984375\n",
      "Precision: 0.9715364507696777, Recall: 0.9351411797595751, F1: 0.952991452991453, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.3359628453527951\n",
      "\n",
      "Iteration: 130, Total Time: 0.19023728370666504\n",
      "Precision: 0.9715447154471545, Recall: 0.9354207436399217, F1: 0.9531405782652044, Accuracy: 0.9104761904761904\n",
      "Current loss: 0.33563450238929143\n",
      "\n",
      "Iteration: 135, Total Time: 0.17801737785339355\n",
      "Precision: 0.9715281812899477, Recall: 0.9348616158792284, F1: 0.9528422852258157, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.3353431308557012\n",
      "\n",
      "Iteration: 140, Total Time: 0.18324637413024902\n",
      "Precision: 0.9718105201976169, Recall: 0.9346003353828954, F1: 0.9528422852258157, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.3350838978569019\n",
      "\n",
      "Iteration: 145, Total Time: 0.24586915969848633\n",
      "Precision: 0.9718105201976169, Recall: 0.9346003353828954, F1: 0.9528422852258157, Accuracy: 0.9099319727891156\n",
      "Current loss: 0.33485268997232065\n",
      "\n",
      "Iteration: 150, Total Time: 0.21136903762817383\n",
      "Precision: 0.972101133391456, Recall: 0.9346186085498742, F1: 0.9529914529914529, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.33464599277816287\n",
      "\n",
      "Iteration: 155, Total Time: 0.13710522651672363\n",
      "Precision: 0.972101133391456, Recall: 0.9346186085498742, F1: 0.9529914529914529, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.3344607929508087\n",
      "\n",
      "Iteration: 160, Total Time: 0.25237369537353516\n",
      "Precision: 0.972101133391456, Recall: 0.9346186085498742, F1: 0.9529914529914529, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.33429449822526663\n",
      "\n",
      "Iteration: 165, Total Time: 0.25855398178100586\n",
      "Precision: 0.972101133391456, Recall: 0.9346186085498742, F1: 0.9529914529914529, Accuracy: 0.9102040816326531\n",
      "Current loss: 0.3341448715808778\n",
      "\n",
      "Iteration: 170, Total Time: 0.2110881805419922\n",
      "Precision: 0.9721092388146426, Recall: 0.9348980162056441, F1: 0.9531405782652043, Accuracy: 0.9104761904761904\n",
      "Current loss: 0.3340099768446072\n",
      "\n",
      "Iteration: 175, Total Time: 0.15917754173278809\n",
      "Precision: 0.9723917465852949, Recall: 0.9346368715083799, F1: 0.9531405782652044, Accuracy: 0.9104761904761904\n",
      "Current loss: 0.3338881335174886\n",
      "\n",
      "Iteration: 180, Total Time: 0.18370509147644043\n",
      "Precision: 0.972682359779134, Recall: 0.9346551242669645, F1: 0.9532896610652236, Accuracy: 0.9107482993197279\n",
      "Current loss: 0.3337778790966798\n",
      "\n",
      "Iteration: 185, Total Time: 0.2160937786102295\n",
      "Precision: 0.972682359779134, Recall: 0.9346551242669645, F1: 0.9532896610652236, Accuracy: 0.9107482993197279\n",
      "Current loss: 0.33367793752305314\n",
      "\n",
      "Iteration: 190, Total Time: 0.16185450553894043\n",
      "Precision: 0.972682359779134, Recall: 0.9346551242669645, F1: 0.9532896610652236, Accuracy: 0.9107482993197279\n",
      "Current loss: 0.3335871926602064\n",
      "\n",
      "Iteration: 195, Total Time: 0.21319913864135742\n",
      "Precision: 0.972980825101685, Recall: 0.9349525404801786, F1: 0.9535876993166286, Accuracy: 0.9112925170068027\n",
      "Current loss: 0.3335046659255578\n",
      "\n",
      "Training stats:\n",
      "Precision: 0.972980825101685, Recall: 0.9349525404801786, F1: 0.9535876993166286, Accuracy: 0.9112925170068027\n",
      "Final training loss: 0.33344398013665616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = logReg.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the model on the 20% test set with 0.5 threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.3917613943399057,\n",
       " 'precision': 0.9543325526932084,\n",
       " 'recall': 0.9188275084554679,\n",
       " 'f1': 0.9362435381964388,\n",
       " 'accuracy': 0.8801295896328294}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.validate(test_set, model['w'], threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix: how to compute the precision/recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(21):\n",
    "    model_stats = logReg.validate(test_set, model['w'], threshold=i/20)\n",
    "    print('{}'.format(model_stats['recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
