{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from DataUtils import DataUtil\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataUtil = DataUtil(sc, 'data/spam.data.txt', 'data/mean_std.txt', True)\n",
    "rdd = dataUtil.read(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWithIndex = rdd.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWithIndex.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWithIndex.filter(lambda x: x[1] % 3 != 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "for i in range(k):\n",
    "    fold = dataWithIndex.filter(lambda x: x[1] % k == i).repartition(sc.defaultParallelism)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(data):\n",
    "    def f(y, pred):\n",
    "        if y == pred:\n",
    "            return 1,0,0,0 if y == 1 else 0,1,0,0\n",
    "        elif pred == 1:\n",
    "            return 0,0,1,0\n",
    "        else:\n",
    "            return 0,0,0,1\n",
    "    tp, tn, fp, fn = data.map(lambda x: f(x[2], x[4])).reduce(lambda a, b: tuple(map(sum, zip(a, b))))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "class ParallelLogReg():\n",
    "    \n",
    "    def __init__(self, sc, dataUtils, iterations, learning_rate, lambda_reg):\n",
    "        self.dataUtils = dataUtils\n",
    "        self.iterations = iterations\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.data = self.dataUtils.read(sc)\n",
    "        self.numberObservations = self.data.count()\n",
    "        self.numberFeatures = 56\n",
    "        \n",
    "    def __add_intercept(self, rdd):\n",
    "        return rdd.map(lambda x: (1, x[0], x[1]))\n",
    "        \n",
    "    def train(self, train_rdd=None, SGD=False, SGD_pct=0.5):\n",
    "        \n",
    "        eps = sys.float_info.epsilon\n",
    "        \n",
    "        # initialize the weights\n",
    "        # w[0]: bias weight\n",
    "        # w[1]: rest of the weights\n",
    "        w = (0, np.zeros(self.numberFeatures))\n",
    "        \n",
    "        if train_rdd == None:\n",
    "            data = self.__add_intercept(self.data)\n",
    "        else:\n",
    "            data = self.__add_intercept(train_rdd)\n",
    "\n",
    "        # initialize prediction to rdd\n",
    "        # x[0]: bias/intercept\n",
    "        # x[1]: rest features\n",
    "        # x[2]: true y\n",
    "        # adding x[3]: predicted y\n",
    "        data = data.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(w[1].dot(x[1]) + w[0] * x[0])))))\n",
    "        numObs = data.count()\n",
    "        print(numObs)\n",
    "        \n",
    "        train = data.cache()\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            start = time.time()\n",
    "            \n",
    "            # sample for SGD\n",
    "            if SGD:\n",
    "                train = data.sample(False, SGD_pct).repartition(4).cache()\n",
    "                if (i==0):\n",
    "                    numObs = train.count()\n",
    "            \n",
    "            # compute derivatives\n",
    "            temp = train.map(lambda x: ((x[3] - x[2]) * x[0], (x[3] - x[2]) * x[1])) \\\n",
    "                       .reduce(lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "            dw = (temp[0]/numObs, (temp[1]/numObs) + (self.lambda_reg/numObs) * w[1])\n",
    "            \n",
    "            # update weights\n",
    "            w = (w[0] - self.lr * dw[0], w[1] - self.lr * dw[1])\n",
    "            \n",
    "            # update prediction\n",
    "            train = train.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(w[1].dot(x[1]) + w[0] * x[0]))))).cache()\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = train.map(lambda x: x[2] * np.log(x[3] + eps) + (1 - x[2]) * np.log(1 - x[3] + eps)) \\\n",
    "                                 .reduce(lambda a,b: a + b)\n",
    "            loss = -(1/numObs) * loss + (self.lambda_reg/(2*numObs)) * np.sum(w[1]**2)\n",
    "            \n",
    "            if (i%10 == 0):\n",
    "                print(\"Current loss: \" + str(loss))\n",
    "                end = time.time()\n",
    "                print(\"Iteration: \" + str(i) + \", Total Time: \" + str(end - start))\n",
    "                \n",
    "        # calculate prediction for entire training set\n",
    "        data = data.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(w[1].dot(x[1]) + w[0] * x[0])))))\n",
    "        \n",
    "        # update loss for entire data set\n",
    "        numObs = data.count()\n",
    "        loss = data.map(lambda x: x[2] * np.log(x[3] + eps) + (1 - x[2]) * np.log(1 - x[3] + eps)) \\\n",
    "                   .reduce(lambda a,b: a + b)\n",
    "        loss = -(1/numObs) * loss + (self.lambda_reg/(2*numObs)) * np.sum(w[1]**2)\n",
    "        \n",
    "        # add the predicted class        \n",
    "        data = data.map(lambda x: (x[0], x[1], x[2], x[3], 1 if x[3] >= 0.5 else 0))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        acc = data.map(lambda x: 1 if x[2] == x[4] else 0) \\\n",
    "                            .reduce(lambda a,b: a+b) / numObs\n",
    "        \n",
    "        print(\"Final training loss: \" + str(loss) + \", Training Accuracy: \" + str(acc))\n",
    "            \n",
    "        return {'loss': loss, 'w': w, 'acc': acc}\n",
    "    \n",
    "    def validate(self, val_rdd, w):\n",
    "        \n",
    "        eps = sys.float_info.epsilon\n",
    "        \n",
    "        val_rdd = self.__add_intercept(val_rdd)\n",
    "        \n",
    "        numObs = val_rdd.count()\n",
    "        \n",
    "        # update prediction\n",
    "        val_rdd = val_rdd.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(w[1].dot(x[1]) + w[0] * x[0])))))\n",
    "        \n",
    "        loss = val_rdd.map(lambda x: x[2] * np.log(x[3] + eps) + (1 - x[2]) * np.log(1 - x[3] + eps)) \\\n",
    "                    .reduce(lambda a,b: a + b)\n",
    "        loss = -(1/numObs) * loss + (self.lambda_reg/(2*numObs)) * np.sum(w[1]**2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def cross_validate(self, k, SGD=False, SGD_pct = 0.5):\n",
    "        \n",
    "        if k == 1:\n",
    "            print(\"Please choose a k > 1. Or use regular train function\")\n",
    "            return\n",
    "        \n",
    "        dataWithIndex = self.data.zipWithIndex();\n",
    "        \n",
    "        losses = np.array([])\n",
    "        models = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            print(\"Fold: \" + str(i))\n",
    "            \n",
    "            fold = dataWithIndex.filter(lambda x: x[1] % k != i).repartition(sc.defaultParallelism) \\\n",
    "                                .map(lambda x: x[0])\n",
    "            \n",
    "            model = self.train(fold, SGD, SGD_pct)\n",
    "            models.append(model)\n",
    "            \n",
    "            #print(model['w'])\n",
    "            \n",
    "            val = dataWithIndex.filter(lambda x: x[1] % k == i).repartition(sc.defaultParallelism) \\\n",
    "                                .map(lambda x: x[0])\n",
    "            \n",
    "            #print(val.first())\n",
    "            losses = np.append(losses, self.validate(val, model['w']))\n",
    "            print(losses)\n",
    "            \n",
    "        print(losses.mean())\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = ParallelLogReg(sc, dataUtil, 100, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg.cross_validate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
