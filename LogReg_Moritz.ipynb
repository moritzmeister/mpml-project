{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from DataUtils import DataUtil\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataUtil = DataUtil(sc, 'data/spam.data.txt', 'data/mean_std.txt', True)\n",
    "rdd = dataUtil.read(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelLogReg():\n",
    "    \n",
    "    def __init__(self, sc, dataUtils, iterations, learning_rate, lambda_reg, fit_intercept):\n",
    "        self.dataUtils = dataUtils\n",
    "        # do we need to broadcast these?\n",
    "        self.iterations = iterations\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.data = self.dataUtils.read(sc)\n",
    "        self.numberObservations = self.data.count()\n",
    "        self.numberFeatures = 56#len(self.data.first()[0])\n",
    "        \n",
    "    def __add_intercept(self):\n",
    "        self.data = self.data.map(lambda x: (1, x[0], x[1]))\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __predict_y(self, w, x):\n",
    "        return self.__sigmoid(w[1].dot(x[1]) + w[0] * x[0])\n",
    "    \n",
    "    def calculateLoss(self):\n",
    "        return 1\n",
    "        \n",
    "    def train(self):\n",
    "        self.__add_intercept()\n",
    "        \n",
    "        # initialize the weights\n",
    "        # w[0]: bias weight\n",
    "        # w[1]: rest of the weights\n",
    "        w = (0, np.zeros(self.numberFeatures))\n",
    "        self.loss = 0\n",
    "        \n",
    "        # initialize prediction to rdd\n",
    "        # x[0]: bias/intercept\n",
    "        # x[1]: rest features\n",
    "        # x[2]: true y\n",
    "        # adding x[3]: predicted y\n",
    "        self.data = self.data.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(w[1].dot(x[1]) + w[0] * x[0])))))\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            # compute derivatives\n",
    "            temp = self.data.map(lambda x: ((x[3] - x[2]) * x[0], (x[3] - x[2]) * x[1])) \\\n",
    "                         .reduce(lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "            self.dw = (temp[0]/self.numberObservations, (temp[1]/self.numberObservations) + (self.lambda_reg/self.numberObservations) * w[1])\n",
    "            \n",
    "            # update weights\n",
    "            w = (w[0] - self.lr * self.dw[0], w[1] - self.lr * self.dw[1])\n",
    "            \n",
    "            # update prediction\n",
    "            self.data = self.data.map(lambda x: (x[0], x[1], x[2], 1 / (1 + np.exp(-(w[1].dot(x[1]) + w[0] * x[0])))))\n",
    "            self.loss = self.data.map(lambda x: x[2] * np.log(x[3]) + (1 - x[2]) * np.log(1 - x[3])) \\\n",
    "                                 .reduce(lambda a,b: a + b)\n",
    "            \n",
    "            self.loss = -(1/self.numberObservations) * self.loss + (self.lambda_reg/(2*self.numberObservations)) * np.sum(w[1]**2)\n",
    "            if (i%10 == 0):\n",
    "                print(self.loss)\n",
    "            \n",
    "        return w\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = ParallelLogReg(sc, dataUtil, 100, 0.01, 0.01, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6886640115564672\n",
      "0.6483598250629653\n",
      "0.6149351436759691\n",
      "0.5868143057599317\n",
      "0.5628412948708029\n",
      "0.5421668988746755\n",
      "0.5241557659059918\n",
      "0.5083237589517262\n",
      "0.4942960005649694\n",
      "0.48177809148641654\n"
     ]
    }
   ],
   "source": [
    "logReg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([-0.34238276,  0.33084612,  0.7127782 , -0.04689292,  0.0115686 ,\n",
       "        -0.35023373, -0.29176885, -0.26252275, -0.32326107, -0.37131646,\n",
       "        -0.29683438,  0.11407682, -0.31201834, -0.174909  , -0.19009813,\n",
       "         0.08615992, -0.32110526,  2.08099864,  0.15088707, -0.16787375,\n",
       "         0.12511659, -0.11815169, -0.29018813, -0.21296991, -0.32878597,\n",
       "        -0.29920177, -0.22786802, -0.23179472, -0.16671161, -0.2252124 ,\n",
       "        -0.16052208, -0.14319455, -0.17490061, -0.14519654, -0.19804231,\n",
       "        -0.2421067 , -0.32341481, -0.05982778, -0.18089355, -0.18528235,\n",
       "        -0.12089424, -0.17258287, -0.20597382, -0.12732885, -0.29773648,\n",
       "        -0.19736149, -0.07138081, -0.11153537, -0.15843841, -0.51424027,\n",
       "        -0.15518786,  0.62394105, -0.3083252 , -0.10303722, -0.04524252,\n",
       "         0.04529222]), 1.0, 0.5005093777015289)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print('loss: {' + str(self.__loss(h, y)) + '} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
