{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtil:\n",
    "    def __init__(self, sc, inputPath='data/spam.data.txt', statsInputPath='data/mean_std.txt', standardize=True):\n",
    "        self.inputPath = inputPath\n",
    "        self.standardize = standardize\n",
    "        self.mean_std = pd.read_csv(statsInputPath, delimiter=',', header=None)\n",
    "        self.mean = sc.broadcast(self.mean_std.iloc[:,0].values.astype(float))\n",
    "        self.std = sc.broadcast(self.mean_std.iloc[:,1].values.astype(float))\n",
    "        \n",
    "        \n",
    "    def read(self, sc):\n",
    "        if self.standardize:\n",
    "            return sc.textFile(self.inputPath).map(lambda x: np.asarray(x.split(' ')).astype(float)) \\\n",
    "                    .map(lambda x: (x[:56], x[57])) \\\n",
    "                    .map(lambda x: ((x[0] - self.mean.value[:56])/self.std.value[:56], x[1]))\n",
    "        else:\n",
    "            return sc.textFile(self.inputPath).map(lambda x: np.asarray(x.split(' ')).astype(float)) \\\n",
    "                    .map(lambda x: (x[:56], x[57]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset: 4601, 56\n",
      "Train dataset: 3675, 56\n",
      "Test dataset: 926, 56\n"
     ]
    }
   ],
   "source": [
    "spam_dataset = DataUtil(sc, 'data/spam.data.txt', 'data/mean_std.txt', True).read(sc)\n",
    "train_set, test_set = spam_dataset.randomSplit(weights=[0.8, 0.2], seed=1)\n",
    "\n",
    "print(\"Whole dataset: {}, {}\".format(spam_dataset.count(), len(spam_dataset.first()[0])))\n",
    "print(\"Train dataset: {}, {}\".format(train_set.count(), len(train_set.first()[0])))\n",
    "print(\"Test dataset: {}, {}\".format(test_set.count(), len(test_set.first()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_function(y, h):\n",
    "    eps = sys.float_info.epsilon\n",
    "    return y * np.log(h + eps) + (1 - y) * np.log(1 - h + eps)\n",
    "\n",
    "def stats(data):\n",
    "    def f(y, pred):\n",
    "        if y == pred:\n",
    "            return 1,0,0,0 if y == 1 else 0,1,0,0\n",
    "        elif pred == 1:\n",
    "            return 0,0,1,0\n",
    "        else:\n",
    "            return 0,0,0,1\n",
    "    tp, tn, fp, fn = data.map(lambda x: f(x[2], x[4])).reduce(lambda a, b: tuple(map(sum, zip(a, b))))\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelLogReg():\n",
    "    \n",
    "    def __init__(self, data, iterations, learning_rate, lambda_reg):\n",
    "        self.data = data\n",
    "        self.numberObservations = self.data.count()\n",
    "        self.numberFeatures = len(self.data.first()[0])\n",
    "        self.iterations = iterations\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "    def __add_intercept(self, rdd):\n",
    "        return rdd.map(lambda x: (1, x[0], x[1]))\n",
    "        \n",
    "    def train(self, train_rdd=None, SGD=False, SGD_pct=0.5, threshold=0.5):\n",
    "        eps = sys.float_info.epsilon\n",
    "        \n",
    "        # initialize the weights\n",
    "        # w[0]: bias weight\n",
    "        # w[1]: rest of the weights\n",
    "        w = (0, np.zeros(self.numberFeatures))\n",
    "        \n",
    "        if train_rdd == None:\n",
    "            data = self.__add_intercept(self.data)\n",
    "        else:\n",
    "            data = self.__add_intercept(train_rdd)\n",
    "\n",
    "        # initialize prediction to rdd\n",
    "        # x[0]: bias/intercept\n",
    "        # x[1]: rest features\n",
    "        # x[2]: true y\n",
    "        # adding x[3]: predicted y\n",
    "        data = data.map(lambda x: (x[0], x[1], x[2], sigmoid(w[1].dot(x[1]) + w[0] * x[0])))\n",
    "        numObs = data.count()\n",
    "        \n",
    "        train = data.cache()\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            start = time.time()\n",
    "            \n",
    "            # sample for SGD\n",
    "            if SGD:\n",
    "                train = data.sample(False, SGD_pct).repartition(sc.defaultParallelism).cache()\n",
    "                if i == 0:\n",
    "                    numObs = train.count()\n",
    "            \n",
    "            # compute derivatives\n",
    "            temp = train.map(lambda x: ((x[3] - x[2]) * x[0], (x[3] - x[2]) * x[1])) \\\n",
    "                       .reduce(lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "            dw = (temp[0]/numObs, (temp[1]/numObs) + self.lambda_reg * np.abs(w[1]))\n",
    "            \n",
    "            # update weights\n",
    "            w = (w[0] - self.lr * dw[0], w[1] - self.lr * dw[1])\n",
    "            \n",
    "            # update prediction\n",
    "            train = train.map(lambda x: (x[0], x[1], x[2], sigmoid(w[1].dot(x[1]) + w[0] * x[0]))).cache()\n",
    "            \n",
    "            if (i % 5 == 0):\n",
    "                end = time.time()\n",
    "                \n",
    "                model_stats = self.validate(train, w, add_intercept=False)\n",
    "\n",
    "                print(\"Iteration: \" + str(i) + \", Total Time: \" + str(end - start))\n",
    "                print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(model_stats['precision'], model_stats['recall'], model_stats['f1'], model_stats['accuracy']))\n",
    "                print('Current loss: {}\\n'.format(model_stats['loss']))\n",
    "                \n",
    "        model_stats = self.validate(data, w, add_intercept=False)\n",
    "        \n",
    "        print('Training stats:')\n",
    "        print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(model_stats['precision'], model_stats['recall'], model_stats['f1'], model_stats['accuracy']))\n",
    "        print('Final training loss: {}\\n'.format(model_stats['loss']))\n",
    "            \n",
    "        return {**model_stats, 'w': w}\n",
    "    \n",
    "    def validate(self, val_rdd, w, threshold=0.5, add_intercept=True):\n",
    "        if add_intercept:\n",
    "            val_rdd = self.__add_intercept(val_rdd)\n",
    "        \n",
    "        numObs = val_rdd.count()\n",
    "        \n",
    "        # update prediction\n",
    "        val_rdd = val_rdd.map(lambda x: (x[0], x[1], x[2], sigmoid(w[1].dot(x[1]) + w[0] * x[0])))\n",
    "        \n",
    "        loss = val_rdd.map(lambda x: cost_function(x[2], x[3])) \\\n",
    "                    .reduce(lambda a,b: a + b)\n",
    "        loss = -(1/numObs) * loss + (self.lambda_reg/2) * np.sum(w[1]**2)\n",
    "        \n",
    "        # calculate stats\n",
    "        val_rdd = val_rdd.map(lambda x: (x[0], x[1], x[2], x[3], 1 if x[3] >= threshold else 0)).cache()\n",
    "        precision, recall, f1, accuracy = stats(val_rdd)\n",
    "        return {'loss': loss, 'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy}\n",
    "    \n",
    "    def cross_validate(self, k, SGD=False, SGD_pct = 0.5):\n",
    "        \n",
    "        if k == 1:\n",
    "            print(\"Please choose a k > 1. Or use regular train function\")\n",
    "            return\n",
    "        \n",
    "        dataWithIndex = self.data.zipWithIndex();\n",
    "        \n",
    "        model_stats = []\n",
    "        models = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            print(\"Fold: \" + str(i))\n",
    "            \n",
    "            fold = dataWithIndex.filter(lambda x: x[1] % k != i).repartition(sc.defaultParallelism) \\\n",
    "                                .map(lambda x: x[0])\n",
    "            \n",
    "            model = self.train(fold, SGD, SGD_pct)\n",
    "            models.append(model)\n",
    "            \n",
    "            val = dataWithIndex.filter(lambda x: x[1] % k == i).repartition(sc.defaultParallelism) \\\n",
    "                                .map(lambda x: x[0])\n",
    "            \n",
    "            val_stats = self.validate(val, model['w'])\n",
    "            model_stats.append(val_stats)\n",
    "            print('Validation stats:')\n",
    "            print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(val_stats['precision'], val_stats['recall'], val_stats['f1'], val_stats['accuracy']))\n",
    "            print('Loss: {}\\n'.format(val_stats['loss']))\n",
    "\n",
    "        avg_model_stats = {}\n",
    "        for key in model_stats[0].keys():\n",
    "            avg_model_stats['avg_' + key] = sum(stat[key] for stat in model_stats) / len(model_stats)\n",
    "\n",
    "        print('Averaged validation stats:')\n",
    "        print('Precision: {}, Recall: {}, F1: {}, Accuracy: {}'.format(avg_model_stats['avg_precision'], avg_model_stats['avg_recall'], avg_model_stats['avg_f1'], avg_model_stats['avg_accuracy']))\n",
    "        print('Loss: {}'.format(avg_model_stats['avg_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = ParallelLogReg(data=train_set, iterations=50, learning_rate=0.2, lambda_reg=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Training stats:\n",
      "Precision: 0.9541745134965474, Recall: 0.9617209743751978, F1: 0.9579328816763826, Accuracy: 0.9192621711521016\n",
      "Final training loss: 0.4155793833480021\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.9385474860335196, Recall: 0.9710982658959537, F1: 0.9545454545454546, Accuracy: 0.9130434782608695\n",
      "Loss: 0.4462898982098461\n",
      "\n",
      "Fold: 1\n",
      "Training stats:\n",
      "Precision: 0.9541745134965474, Recall: 0.9617209743751978, F1: 0.9579328816763826, Accuracy: 0.9192621711521016\n",
      "Final training loss: 0.42126326573826345\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.9473684210526315, Recall: 0.9799426934097422, F1: 0.9633802816901408, Accuracy: 0.9293478260869565\n",
      "Loss: 0.4107424088659815\n",
      "\n",
      "Fold: 2\n",
      "Training stats:\n",
      "Precision: 0.9520676691729323, Recall: 0.9635383639822448, F1: 0.9577686731799558, Accuracy: 0.9189597822800121\n",
      "Final training loss: 0.42215690599657574\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.957983193277311, Recall: 0.9688385269121813, F1: 0.9633802816901408, Accuracy: 0.9293478260869565\n",
      "Loss: 0.41561123258516497\n",
      "\n",
      "Fold: 3\n",
      "Training stats:\n",
      "Precision: 0.9545739348370927, Recall: 0.9636306135357369, F1: 0.9590808939250866, Accuracy: 0.9213788932567282\n",
      "Final training loss: 0.41974171243966096\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.9352112676056338, Recall: 0.9623188405797102, F1: 0.9485714285714286, Accuracy: 0.9021739130434783\n",
      "Loss: 0.43682602852801744\n",
      "\n",
      "Fold: 4\n",
      "Training stats:\n",
      "Precision: 0.9533792240300375, Recall: 0.9648511716276124, F1: 0.9590808939250864, Accuracy: 0.9213788932567282\n",
      "Final training loss: 0.4198728160684556\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.963276836158192, Recall: 0.9605633802816902, F1: 0.9619181946403386, Accuracy: 0.9266304347826086\n",
      "Loss: 0.4214599516985026\n",
      "\n",
      "Fold: 5\n",
      "Training stats:\n",
      "Precision: 0.9552145317882869, Recall: 0.9636650868878357, F1: 0.9594212016357345, Accuracy: 0.9220072551390568\n",
      "Final training loss: 0.42211914227866376\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.9466292134831461, Recall: 0.9683908045977011, F1: 0.9573863636363636, Accuracy: 0.9182561307901907\n",
      "Loss: 0.40392053704134706\n",
      "\n",
      "Fold: 6\n",
      "Training stats:\n",
      "Precision: 0.9530810134501094, Recall: 0.9648511716276124, F1: 0.9589299763965381, Accuracy: 0.9211003627569528\n",
      "Final training loss: 0.42136067220252665\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.9685714285714285, Recall: 0.952247191011236, F1: 0.9603399433427763, Accuracy: 0.9237057220708447\n",
      "Loss: 0.41140464873317073\n",
      "\n",
      "Fold: 7\n",
      "Training stats:\n",
      "Precision: 0.9527977492966552, Recall: 0.9654735508394044, F1: 0.9590937696664568, Accuracy: 0.9214026602176542\n",
      "Final training loss: 0.419535482617967\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.9542857142857143, Recall: 0.9515669515669516, F1: 0.9529243937232527, Accuracy: 0.9100817438692098\n",
      "Loss: 0.4383361387972966\n",
      "\n",
      "Fold: 8\n",
      "Training stats:\n",
      "Precision: 0.9530075187969925, Recall: 0.9632678910702976, F1: 0.9581102362204725, Accuracy: 0.9195888754534461\n",
      "Final training loss: 0.41655276522075235\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.9628571428571429, Recall: 0.9519774011299436, F1: 0.9573863636363636, Accuracy: 0.9182561307901907\n",
      "Loss: 0.42915355276714356\n",
      "\n",
      "Fold: 9\n",
      "Training stats:\n",
      "Precision: 0.9539040451552211, Recall: 0.9623536855425499, F1: 0.9581102362204724, Accuracy: 0.9195888754534461\n",
      "Final training loss: 0.41947331926865133\n",
      "\n",
      "Validation stats:\n",
      "Precision: 0.940677966101695, Recall: 0.9624277456647399, F1: 0.9514285714285715, Accuracy: 0.9073569482288828\n",
      "Loss: 0.4321498286346603\n",
      "\n",
      "Averaged validation stats:\n",
      "Precision: 0.9515408669426417, Recall: 0.962937180104985, F1: 0.9571261276904831, Accuracy: 0.9178200154010188\n",
      "Loss: 0.42458942258611315\n",
      "Total time:\n",
      "151.76130962371826\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "logReg.cross_validate(10)\n",
    "print('Total time:')\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stats:\n",
      "Precision: 0.9540456723992106, Recall: 0.9635535307517085, F1: 0.958776030599235, Accuracy: 0.9208163265306123\n",
      "Final training loss: 0.4199384531598891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = logReg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.998003992015968\n",
      "0.9944649446494465\n",
      "0.993322203672788\n",
      "0.9937106918238994\n",
      "0.9897810218978103\n",
      "0.989100817438692\n",
      "0.9842931937172775\n",
      "0.9792682926829268\n",
      "0.9660421545667447\n",
      "0.9586206896551724\n",
      "0.9378531073446328\n",
      "0.9178852643419573\n",
      "0.9049217002237137\n",
      "0.8694690265486725\n",
      "0.8371837183718371\n",
      "0.812910284463895\n",
      "0.7726775956284153\n",
      "0.7442872687704026\n",
      "0.6916395222584147\n",
      "0.6198704103671706\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "    model_stats = logReg.validate(test_set, model['w'], threshold=i/20)\n",
    "    print('{}'.format(model_stats['recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
